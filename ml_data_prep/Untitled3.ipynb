{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a0274f-88e9-44e4-b3cf-40bdfffe72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tqdm\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy import signal\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class QuakeData:\n",
    "\n",
    "    def __init__(self,\n",
    "                 chunk_data=False,\n",
    "                 window_size=1000,\n",
    "                 window_step_size=100,\n",
    "                 window_deadzone=200,\n",
    "                 downsample_data=False,\n",
    "                 downsample_hz=6.625,\n",
    "                 normalize_data=False,\n",
    "                 normalize_method='z-score',\n",
    "                 use_gauss_labels=False,\n",
    "                 gauss_factor=0.5,\n",
    "                 use_neg_one_for_deadzone=False,\n",
    "                 use_earth=False\n",
    "            ):\n",
    "        self.train_data = {}\n",
    "        self.test_data = {}\n",
    "\n",
    "        self.chunk_data = chunk_data\n",
    "        self.window_size = window_size\n",
    "        self.window_step_size = window_step_size\n",
    "        self.window_deadzone = window_deadzone\n",
    "\n",
    "        self.normalize_data = normalize_data\n",
    "        self.normalize_method = normalize_method\n",
    "        assert self.normalize_method in {'z-score'}\n",
    "\n",
    "        self.use_gauss_labels = use_gauss_labels\n",
    "        self.gauss_factor = gauss_factor\n",
    "        self.use_earth = use_earth\n",
    "\n",
    "        self.use_neg_one_for_deadzone = use_neg_one_for_deadzone\n",
    "\n",
    "        self.train_data['lunar'] = []\n",
    "        self.train_data['mars'] = []\n",
    "        self.train_data['earth'] = []\n",
    "        self.train_indices = []\n",
    "\n",
    "        self.test_data['lunar'] = []\n",
    "        self.test_data['mars'] = []\n",
    "        self.test_indices = []\n",
    "\n",
    "        base_data_path = \"./space_apps_2024_seismic_detection/data/\"\n",
    "        \n",
    "        lunar_cat_path = os.path.join(base_data_path, \"lunar\", \"training\", \"catalogs\", 'apollo12_catalog_GradeA_final.csv')\n",
    "        lunar_data_path = os.path.join(base_data_path, \"lunar\", \"training\", \"data\", \"S12_GradeA\")\n",
    "\n",
    "        lunar_files = self.process_cat_file(lunar_cat_path, lunar_data_path)\n",
    "        l = [x['mseed_path'] for x in lunar_files]\n",
    "        assert len(l) == len(set(l))\n",
    "        for x in tqdm.tqdm(lunar_files):\n",
    "            x_stats, x_t, x_v = self.read_mseed(x['mseed_path'])\n",
    "\n",
    "\n",
    "            if downsample_data:\n",
    "                x_t, x_v = self.downsample_data(x_t, x_v, x_stats, hz=downsample_hz)\n",
    "\n",
    "            label_arr = np.zeros_like(x_t)\n",
    "            i = np.abs(x_t - x['label']).argmin()\n",
    "            label_arr[i] = 1\n",
    "            \n",
    "            self.train_data['lunar'].append({\n",
    "                'stats':x_stats,\n",
    "                'values':x_v,\n",
    "                'times':x_t,\n",
    "                'label':x['label'],\n",
    "                'label_arr':label_arr\n",
    "            })\n",
    "\n",
    "        for file_index, x in enumerate(self.train_data['lunar']):\n",
    "            n = len(x['values'])\n",
    "\n",
    "            if self.chunk_data:\n",
    "                for i in range(0, n-window_size, window_step_size):\n",
    "                    self.train_indices.append(('lunar', file_index, i))\n",
    "            else:\n",
    "                self.train_indices.append(('lunar', file_index, 0))\n",
    "\n",
    "\n",
    "        mars_cat_path = os.path.join(base_data_path, \"mars\", \"training\", \"catalogs\", 'Mars_InSight_training_catalog_final.csv')\n",
    "        mars_data_path = os.path.join(base_data_path, \"mars\", \"training\", \"data\")\n",
    "        mars_files = self.process_cat_file(mars_cat_path, mars_data_path)\n",
    "        l = [x['mseed_path'] for x in mars_files]\n",
    "        assert len(l) == len(set(l))\n",
    "        for x in tqdm.tqdm(mars_files):\n",
    "            x_stats, x_t, x_v = self.read_mseed(x['mseed_path'])\n",
    "\n",
    "            if downsample_data:\n",
    "                x_t, x_v = self.downsample_data(x_t, x_v, x_stats, hz=downsample_hz)\n",
    "\n",
    "            label_arr = np.zeros_like(x_t)\n",
    "            i = np.abs(x_t - x['label']).argmin()\n",
    "            label_arr[i] = 1\n",
    "\n",
    "            print(len(x_v))\n",
    "\n",
    "            self.train_data['mars'].append({\n",
    "                'stats':x_stats,\n",
    "                'values':x_v,\n",
    "                'times':x_t,\n",
    "                'label':x['label'],\n",
    "                'label_arr':label_arr\n",
    "            })\n",
    "\n",
    "        for file_index, x in enumerate(self.train_data['mars']):\n",
    "            n = len(x['values'])\n",
    "\n",
    "            if self.chunk_data:\n",
    "                for i in range(0, n-window_size, window_step_size):\n",
    "                    self.train_indices.append(('mars', file_index, i))\n",
    "            else:\n",
    "                self.train_indices.append(('mars', file_index, 0))\n",
    "\n",
    "\n",
    "\n",
    "        lunar_test_root = os.path.join(base_data_path, \"lunar\", \"test\")\n",
    "        for mseed_path in self.get_all_mseed(lunar_test_root):\n",
    "            x_stats, x_t, x_v = self.read_mseed(mseed_path)\n",
    "\n",
    "            if downsample_data:\n",
    "                x_t, x_v = self.downsample_data(x_t, x_v, x_stats, hz=downsample_hz)\n",
    "\n",
    "            self.test_data['lunar'].append({\n",
    "                'stats':x_stats,\n",
    "                'values':x_v,\n",
    "                'times':x_t\n",
    "            })\n",
    "\n",
    "        for file_index, x in enumerate(self.test_data['lunar']):\n",
    "            n = len(x['values'])\n",
    "\n",
    "            if self.chunk_data:\n",
    "                for i in range(0, n-window_size, window_step_size):\n",
    "                    self.test_indices.append(('lunar', file_index, i))\n",
    "            else:\n",
    "                self.test_indices.append(('lunar', file_index, 0))\n",
    "\n",
    "        mars_test_root = os.path.join(base_data_path, \"mars\", \"test\")\n",
    "        for mseed_path in self.get_all_mseed(mars_test_root):\n",
    "            x_stats, x_t, x_v = self.read_mseed(mseed_path)\n",
    "\n",
    "            if downsample_data:\n",
    "                x_t, x_v = self.downsample_data(x_t, x_v, x_stats, hz=downsample_hz)\n",
    "\n",
    "            self.test_data['mars'].append({\n",
    "                'stats':x_stats,\n",
    "                'values':x_v,\n",
    "                'times':x_t\n",
    "            })\n",
    "\n",
    "        for file_index, x in enumerate(self.test_data['mars']):\n",
    "            n = len(x['values'])\n",
    "\n",
    "            if self.chunk_data:\n",
    "                for i in range(0, n-window_size, window_step_size):\n",
    "                    self.test_indices.append(('mars', file_index, i))\n",
    "            else:\n",
    "                self.test_indices.append(('mars', file_index, 0))\n",
    "\n",
    "        if self.use_earth:\n",
    "            base_earth_dir = \"./earth_train_data/\"\n",
    "            for f_name in tqdm.tqdm(os.listdir(base_earth_dir)):\n",
    "                f_path = os.path.join(base_earth_dir, f_name)\n",
    "                x_stats, x_t, x_v = self.read_mseed(f_path)\n",
    "    \n",
    "                if downsample_data:\n",
    "                    x_t, x_v = self.downsample_data(x_t, x_v, x_stats, hz=downsample_hz)\n",
    "    \n",
    "                label_arr = np.zeros_like(x_t)\n",
    "                i = np.abs(x_t - 3000).argmin()\n",
    "                label_arr[i] = 1\n",
    "    \n",
    "                #print(x_t[0], x_t[-1])\n",
    "    \n",
    "                self.train_data['earth'].append({\n",
    "                    'stats':x_stats,\n",
    "                    'values':x_v,\n",
    "                    'times':x_t,\n",
    "                    'label':3000,\n",
    "                    'label_arr':label_arr\n",
    "                })\n",
    "    \n",
    "            for file_index, x in enumerate(self.train_data['earth']):\n",
    "                n = len(x['values'])\n",
    "    \n",
    "                if self.chunk_data:\n",
    "                    for i in range(0, n-window_size, window_step_size):\n",
    "                        self.train_indices.append(('earth', file_index, i))\n",
    "                else:\n",
    "                    self.train_indices.append(('earth', file_index, 0))\n",
    "\n",
    "    \n",
    "    def add_synthetic_data(self, num_pos=100):\n",
    "        #train_indices.append(('lunar', file_index, 0))\n",
    "\n",
    "        x = {}\n",
    "        k_f_l = set()\n",
    "        \n",
    "        for i, (key, file_index, _) in enumerate(self.train_indices):\n",
    "            if key not in {'mars', 'lunar'}:\n",
    "                continue\n",
    "\n",
    "            if key not in x:\n",
    "                x[key] = {}\n",
    "            if file_index not in x[key]:\n",
    "                x[key][file_index] = {'pos':[], 'neg':[]}\n",
    "\n",
    "            _, _, has_sample, has_sample_at_all = self.get_train_sample(i)\n",
    "\n",
    "            if has_sample:\n",
    "                x[key][file_index]['pos'].append(i)\n",
    "            elif not has_sample_at_all:\n",
    "                x[key][file_index]['neg'].append(i)\n",
    "\n",
    "            k_f_l.add((key, file_index))\n",
    "        # random select key/fileindex\n",
    "        # random select pos case to add to a random neg case\n",
    "        # repeat\n",
    "\n",
    "        for k in x:\n",
    "            for f in x[k]:\n",
    "                print(k, f, len(x[k][f]['pos']), len(x[k][f]['neg']))\n",
    "\n",
    "        k_f_l = list(k_f_l)\n",
    "\n",
    "        self.train_data['synthetic'] = []\n",
    "        for i in tqdm.tqdm(range(num_pos)):\n",
    "            key, file_index = random.choice(k_f_l)\n",
    "\n",
    "            pos_i = random.choice(x[key][file_index]['pos'])\n",
    "            neg_i = random.choice(x[key][file_index]['neg'])\n",
    "\n",
    "            x_v_pos, label_arr, _, _ = self.get_train_sample(pos_i)\n",
    "            x_v_neg, _, _, _ = self.get_train_sample(neg_i)\n",
    "\n",
    "            x_v_neg = x_v_neg / np.abs(x_v_neg).max()\n",
    "\n",
    "            x_v = x_v_pos + x_v_neg\n",
    "\n",
    "            if self.normalize_data:\n",
    "                if self.normalize_method=='z-score':\n",
    "                    x_v = self.z_norm(x_v)\n",
    "\n",
    "            self.train_data['synthetic'].append((x_v, label_arr, x_v_pos, x_v_neg))\n",
    "            self.train_indices.append(('synthetic', None, i))\n",
    "\n",
    "    def get_train_sample(self, index, return_extra_synthetic_data=False):\n",
    "        key, file_index, i = self.train_indices[index]\n",
    "\n",
    "        if key=='synthetic':\n",
    "            x_v, label_arr, x_v_pos, x_v_neg = self.train_data['synthetic'][i]\n",
    "            label_arr = self.transform_labels(label_arr)\n",
    "            \n",
    "            if return_extra_synthetic_data:\n",
    "                return x_v, label_arr, True, True, x_v_pos, x_v_neg\n",
    "            else:\n",
    "                return x_v, label_arr, True, True\n",
    "\n",
    "        x_v = self.train_data[key][file_index]['values']\n",
    "        label_arr = self.train_data[key][file_index]['label_arr']\n",
    "\n",
    "        if self.chunk_data:\n",
    "            j = i + self.window_size\n",
    "            x_v = x_v[i:j]\n",
    "            label_arr = np.copy(label_arr[i:j])\n",
    "\n",
    "            has_sample_at_all = (label_arr > 0.99999).any()\n",
    "\n",
    "            label_arr[:self.window_deadzone] = 0\n",
    "            label_arr[-self.window_deadzone:] = 0\n",
    "\n",
    "            has_sample = (label_arr > 0.99999).any()\n",
    "        else:\n",
    "            has_sample = (label_arr > 0.99999).any()\n",
    "            has_sample_at_all = (label_arr > 0.99999).any()\n",
    "\n",
    "        if self.normalize_data:\n",
    "            if self.normalize_method=='z-score':\n",
    "                x_v = self.z_norm(x_v)\n",
    "\n",
    "        label_arr = self.transform_labels(label_arr)\n",
    "        return x_v, label_arr, has_sample, has_sample_at_all\n",
    "\n",
    "    def transform_labels(self, label_arr):\n",
    "        #self.use_gauss_labels = use_gauss_labels\n",
    "        #self.gause_factor = gause_factor\n",
    "        #self.use_neg_one_for_deadzone = use_neg_one_for_deadzone\n",
    "        label_arr = np.copy(label_arr).astype(float)\n",
    "\n",
    "        if self.use_gauss_labels and (label_arr > 0.99999).any():\n",
    "            assert (label_arr > 0.99999).sum() == 1\n",
    "            i = label_arr.argmax()\n",
    "            v = self.gauss_factor\n",
    "            for j in range(i+1, len(label_arr)):\n",
    "                label_arr[j] = v\n",
    "                v *= self.gauss_factor\n",
    "\n",
    "            v = self.gauss_factor\n",
    "            for j in range(i-1, -1, -1):\n",
    "                label_arr[j] = v\n",
    "                v *= self.gauss_factor\n",
    "\n",
    "        if self.use_neg_one_for_deadzone:\n",
    "            label_arr[:self.window_deadzone] = -1\n",
    "            label_arr[-self.window_deadzone:] = -1\n",
    "\n",
    "        return label_arr\n",
    "\n",
    "    \n",
    "    def get_test_sample(self, index):\n",
    "        key, file_index, i = self.test_indices[index]\n",
    "\n",
    "        x_v = self.test_data[key][file_index]['values']\n",
    "        if self.chunk_data:\n",
    "            j = i + self.window_size\n",
    "            x_v = x_v[i:j]\n",
    "\n",
    "        if self.normalize_data:\n",
    "            if self.normalize_method=='z-score':\n",
    "                x_v = self.z_norm(x_v)\n",
    "\n",
    "        return x_v\n",
    "\n",
    "    def create_val_split(self, p=0.2):\n",
    "        neg_indices = []\n",
    "        pos_indices = []\n",
    "\n",
    "        for i in range(self.num_train_samples()):\n",
    "            _, _, has_sample, _ = self.get_train_sample(i)\n",
    "\n",
    "            if has_sample:\n",
    "                pos_indices.append(i)\n",
    "            else:\n",
    "                neg_indices.append(i)\n",
    "\n",
    "        train_indices = []\n",
    "        dev_indices = []\n",
    "\n",
    "        random.seed(1234)\n",
    "        random.shuffle(pos_indices)\n",
    "        random.shuffle(neg_indices)\n",
    "\n",
    "        i = math.ceil(p * len(pos_indices))\n",
    "        dev_indices += pos_indices[:i]\n",
    "        train_indices += pos_indices[i:]\n",
    "\n",
    "        i = math.ceil(p * len(neg_indices))\n",
    "        dev_indices += neg_indices[:i]\n",
    "        train_indices += neg_indices[i:]\n",
    "\n",
    "        return train_indices, dev_indices\n",
    "\n",
    "    \n",
    "    def num_train_samples(self):\n",
    "        return len(self.train_indices)\n",
    "\n",
    "    def get_train_indices_by_key(self, k):\n",
    "        l = []\n",
    "        #key, file_index, i = self.train_indices[index]\n",
    "        for i in range(len(self.train_indices)):\n",
    "            key = self.train_indices[i][0]\n",
    "            if key == k:\n",
    "                l.append(i)\n",
    "\n",
    "        return l\n",
    "\n",
    "    def process_cat_file(self, cat_path, data_root_path):\n",
    "        cat_df = pd.read_csv(cat_path)\n",
    "\n",
    "        l = []\n",
    "        for x in cat_df.to_dict(orient='records'):\n",
    "            x_filename = x['filename'].replace(\".csv\", \"\").replace(\".mseed\", \"\")\n",
    "            mseed_path = os.path.join(data_root_path, x_filename + \".mseed\")\n",
    "            \n",
    "            if not os.path.isfile(mseed_path):\n",
    "                print('not found, skipping: ', x['filename'], mseed_path)\n",
    "                continue\n",
    "\n",
    "            v = {\n",
    "                'mseed_path' : mseed_path,\n",
    "                'label' : x['time_rel(sec)']\n",
    "            }\n",
    "\n",
    "            v['mq_type'] = x.get('mq_type', 'n/a')\n",
    "\n",
    "            l.append(v)\n",
    "\n",
    "        return l\n",
    "\n",
    "    def read_mseed(self, mseed_path):\n",
    "        x = read(mseed_path)\n",
    "\n",
    "        stats = x[0].stats\n",
    "        x_t = x.traces[0].times()\n",
    "        x_v = x.traces[0].data\n",
    "\n",
    "        return (stats, x_t, x_v)\n",
    "\n",
    "    def get_all_mseed(self, root_path):\n",
    "        f_path_l = []\n",
    "        for path, subdirs, files in os.walk(root_path):\n",
    "            for f in files:\n",
    "                if '.mseed' not in f: continue\n",
    "                f_path_l.append(os.path.join(path, f))\n",
    "\n",
    "        return f_path_l\n",
    "    \n",
    "            \n",
    "    def downsample_data(self, x_t, x_v, x_stats, hz=6.625):\n",
    "\n",
    "        x_hz = x_stats.sampling_rate\n",
    "\n",
    "        if abs(x_hz - hz) < 0.1:\n",
    "            return x_t, x_v\n",
    "\n",
    "        #print(f\"downsampling from {x_hz} to {hz}\")\n",
    "\n",
    "        nyquist_freq = hz / 2.0\n",
    "        b, a = butter(N=4, Wn=nyquist_freq, btype='low', fs=x_hz)\n",
    "        x_v_filtered = signal.filtfilt(b, a, x_v)\n",
    "\n",
    "        downsample_ratio = x_hz / hz\n",
    "        new_num_samples = int(np.round(len(x_t) / downsample_ratio))\n",
    "        x_v_downsampled, x_t_downsampled = signal.resample(x_v_filtered, new_num_samples, t=x_t)\n",
    "\n",
    "        return x_t_downsampled, x_v_downsampled\n",
    "\n",
    "    def z_norm(self, x):\n",
    "        x = np.copy(x)\n",
    "        return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#q = QuakeData(downsample_data=True, chunk_data=True, normalize_data=True, window_step_size=5_000, window_size=50_000, window_deadzone=5_000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eeec26-735f-4a28-857f-cf2739678857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found, skipping:  xa.s12.00.mhz.1971-04-13HR00_evid00029 ./space_apps_2024_seismic_detection/data/lunar/training/data/S12_GradeA/xa.s12.00.mhz.1971-04-13HR00_evid00029.mseed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 75/75 [00:04<00:00, 15.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23850\n",
      "23850\n",
      "lunar 0 8 95\n",
      "lunar 1 8 95\n",
      "lunar 2 8 95\n",
      "lunar 3 5 99\n",
      "lunar 4 8 95\n",
      "lunar 5 8 95\n",
      "lunar 6 8 95\n",
      "lunar 7 8 95\n",
      "lunar 8 8 95\n",
      "lunar 9 8 95\n",
      "lunar 10 8 95\n",
      "lunar 11 8 95\n",
      "lunar 12 8 95\n",
      "lunar 13 8 95\n",
      "lunar 14 8 95\n",
      "lunar 15 8 95\n",
      "lunar 16 8 95\n",
      "lunar 17 8 95\n",
      "lunar 18 8 95\n",
      "lunar 19 8 95\n",
      "lunar 20 8 95\n",
      "lunar 21 8 95\n",
      "lunar 22 8 95\n",
      "lunar 23 8 95\n",
      "lunar 24 7 97\n",
      "lunar 25 8 95\n",
      "lunar 26 8 95\n",
      "lunar 27 8 95\n",
      "lunar 28 8 95\n",
      "lunar 29 8 95\n",
      "lunar 30 8 95\n",
      "lunar 31 8 95\n",
      "lunar 32 8 95\n",
      "lunar 33 8 95\n",
      "lunar 34 8 95\n",
      "lunar 35 8 95\n",
      "lunar 36 8 95\n",
      "lunar 37 8 95\n",
      "lunar 38 8 95\n",
      "lunar 39 8 95\n",
      "lunar 40 8 95\n",
      "lunar 41 4 100\n",
      "lunar 42 3 101\n",
      "lunar 43 8 95\n",
      "lunar 44 8 49\n",
      "lunar 45 8 95\n",
      "lunar 46 3 101\n",
      "lunar 47 8 95\n",
      "lunar 48 8 95\n",
      "lunar 49 8 96\n",
      "lunar 50 8 95\n",
      "lunar 51 8 95\n",
      "lunar 52 8 95\n",
      "lunar 53 8 95\n",
      "lunar 54 4 100\n",
      "lunar 55 1 103\n",
      "lunar 56 8 95\n",
      "lunar 57 2 102\n",
      "lunar 58 8 95\n",
      "lunar 59 6 98\n",
      "lunar 60 8 95\n",
      "lunar 61 8 95\n",
      "lunar 62 8 95\n",
      "lunar 63 8 95\n",
      "lunar 64 8 95\n",
      "lunar 65 1 103\n",
      "lunar 66 8 95\n",
      "lunar 67 8 95\n",
      "lunar 68 4 100\n",
      "lunar 69 8 95\n",
      "lunar 70 8 95\n",
      "lunar 71 8 95\n",
      "lunar 72 8 95\n",
      "lunar 73 8 95\n",
      "lunar 74 8 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▋                                                                  | 1202/8000 [00:07<00:43, 157.96it/s]"
     ]
    }
   ],
   "source": [
    "q = QuakeData(\n",
    "    downsample_data=True,\n",
    "    chunk_data=True,\n",
    "    normalize_data=True,\n",
    "    window_step_size=5_000,\n",
    "    window_size=50_000,\n",
    "    window_deadzone=5_000,\n",
    "    use_neg_one_for_deadzone=False,\n",
    "    use_earth=False,\n",
    "    use_gauss_labels=True,\n",
    "    gauss_factor=0.98,\n",
    ")\n",
    "\n",
    "q.add_synthetic_data(8000)\n",
    "\n",
    "train_indices, val_indices = q.create_val_split(p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7a282-c43e-43a9-998a-95ac1b73f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class QuakeTorchDataset(Dataset):\n",
    "    def __init__(self, quake_data, indices):\n",
    "        super().__init__()\n",
    "\n",
    "        self.quake_data = quake_data\n",
    "        self.indices = indices\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "\n",
    "        x_v, label_arr, _, _ = self.quake_data.get_train_sample(i)\n",
    "\n",
    "        x_v = torch.from_numpy(x_v)\n",
    "        label_arr = torch.from_numpy(label_arr)\n",
    "\n",
    "        return (x_v, label_arr)\n",
    "\n",
    "\n",
    "train_dataset = QuakeTorchDataset(q, train_indices)\n",
    "val_dataset = QuakeTorchDataset(q, val_indices)\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227af47d-c94c-4ebe-8895-e1894f1cbe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Wave_Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dilation_rates, kernel_size):\n",
    "        super(Wave_Block, self).__init__()\n",
    "        self.num_rates = dilation_rates\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n",
    "        dilation_rates = [2 ** i for i in range(dilation_rates)]\n",
    "        for dilation_rate in dilation_rates:\n",
    "            self.filter_convs.append(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n",
    "            self.gate_convs.append(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n",
    "            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs[0](x)\n",
    "        res = x\n",
    "        for i in range(self.num_rates):\n",
    "            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n",
    "            x = self.convs[i + 1](x)\n",
    "            res = res + x\n",
    "        return res\n",
    "# detail \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, inch=1, kernel_size=3):\n",
    "        super().__init__()\n",
    "        #self.LSTM = nn.GRU(input_size=input_size, hidden_size=64, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.wave_block1 = Wave_Block(inch, 8, 8, kernel_size)\n",
    "        self.wave_block2 = Wave_Block(8, 16, 4, kernel_size)\n",
    "        self.wave_block3 = Wave_Block(16, 32, 2, kernel_size)\n",
    "        #self.wave_block4 = Wave_Block(64, 128, 1, kernel_size)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.wave_block1(x)\n",
    "        x = self.wave_block2(x)\n",
    "        x = self.wave_block3(x)\n",
    "\n",
    "        #x = self.wave_block4(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        #x, _ = self.LSTM(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Classifier()\n",
    "\n",
    "print(model(torch.rand((4, 10000, 1))).shape)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e340c1-cdb9-4d26-9e1b-27b8180076cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "model = Classifier()\n",
    "model = model.cuda()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.BCELoss(reduction='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e30ee-5a45-48e9-a11e-47b1b4ae5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "for epoch_index in range(10):\n",
    "    l = []\n",
    "    model = model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "    \n",
    "        optim.zero_grad()\n",
    "    \n",
    "        inputs = inputs.unsqueeze(2).cuda().float()\n",
    "        labels = labels.cuda().float()\n",
    "    \n",
    "        y = model(inputs).squeeze(2)\n",
    "        y = nn.functional.sigmoid(y)\n",
    "        loss = loss_fn(y, labels) / inputs.shape[0]\n",
    "    \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        l.append(loss.item())\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print(loss.item())\n",
    "\n",
    "    v = sum(l) / len(l)\n",
    "    print('epoch', v)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        l = []\n",
    "        for i in tqdm.tqdm(val_indices):\n",
    "            x_v, label_arr, has_pos, has_pos_anywhere = q.get_train_sample(i)\n",
    "    \n",
    "            x_tensor = torch.tensor(x_v).float().unsqueeze(0).unsqueeze(2).cuda()\n",
    "            label_tensor = torch.tensor(label_arr).float().unsqueeze(0).cuda()\n",
    "    \n",
    "            y = model(x_tensor).squeeze(2)\n",
    "            y = nn.functional.sigmoid(y)\n",
    "    \n",
    "            loss = loss_fn(y, label_tensor)\n",
    "            l.append(loss)\n",
    "\n",
    "        v = sum(l) / len(l)\n",
    "        print(\"eval\", v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e586522-c9b1-4394-a4e9-7bd29a84c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f74198-0323-416d-9f85-458f1c17df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "l = []\n",
    "l2 = []\n",
    "for i in tqdm.tqdm(val_indices):\n",
    "    x_v, label_arr, has_pos, has_pos_anywhere = q.get_train_sample(i)\n",
    "    \n",
    "    if has_pos:\n",
    "        j = label_arr.argmax()\n",
    "    \n",
    "        x_tensor = torch.tensor(x_v).float().unsqueeze(0).unsqueeze(2).cuda()\n",
    "        y = nn.functional.sigmoid(model(x_tensor.detach()))\n",
    "    \n",
    "        k = y.argmax().item()\n",
    "    \n",
    "        l.append(abs(k - j))\n",
    "    else:\n",
    "        x_tensor = torch.tensor(x_v).float().unsqueeze(0).unsqueeze(2).cuda()\n",
    "        y = nn.functional.sigmoid(model(x_tensor.detach()))\n",
    "\n",
    "        k = y.max().item()\n",
    "\n",
    "        l2.append(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a7f20-756d-41f4-8128-bd74406d5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sum(l) / len(l)\n",
    "v\n",
    "print(min(l), max(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a00f0e-e187-42d8-abe7-a5a51114fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
